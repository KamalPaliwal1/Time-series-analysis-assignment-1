{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q1. What is a time series, and what are some common applications of time series analysis?\n",
    "✅ What is a Time Series?\n",
    "A time series is a sequence of data points collected or recorded at regular time intervals. It tracks changes over time, where the order of the data is crucial. Each observation is recorded sequentially, often at consistent intervals like hourly, daily, monthly, or yearly.\n",
    "📈 Key Characteristics:\n",
    "•\tTemporal Dependence: Data points are dependent on previous values.\n",
    "•\tTrend: Overall direction of the data over time (upward, downward, or stationary).\n",
    "•\tSeasonality: Regular, repeating patterns or fluctuations at specific intervals.\n",
    "•\tNoise/Irregularities: Random variations that are not part of the trend or seasonality.\n",
    "________________________________________\n",
    "⚡️ Common Applications of Time Series Analysis:\n",
    "1.\t📊 Financial Market Analysis\n",
    "o\tStock price prediction\n",
    "o\tPortfolio risk assessment\n",
    "o\tVolatility modeling (e.g., using ARIMA or GARCH models)\n",
    "2.\t🏢 Business and Sales Forecasting\n",
    "o\tDemand and inventory forecasting\n",
    "o\tRevenue and sales prediction\n",
    "o\tMarket trend analysis\n",
    "3.\t⏰ Operational Monitoring and Anomaly Detection\n",
    "o\tNetwork traffic analysis for identifying cyber threats\n",
    "o\tSystem health monitoring for predictive maintenance\n",
    "o\tFraud detection in financial transactions\n",
    "4.\t🌦️ Weather and Climate Modeling\n",
    "o\tTemperature forecasting\n",
    "o\tRainfall prediction\n",
    "o\tClimate pattern analysis (El Niño, etc.)\n",
    "5.\t📡 IoT and Sensor Data Analytics\n",
    "o\tMonitoring equipment performance\n",
    "o\tAnomaly detection in industrial sensors\n",
    "o\tEnergy consumption analysis\n",
    "6.\t🏥 Healthcare and Epidemiology\n",
    "o\tPatient vital signs monitoring\n",
    "o\tDisease outbreak modeling (e.g., COVID-19 spread)\n",
    "o\tPrediction of hospital admissions\n",
    "7.\t🧠 Natural Language Processing (NLP) and Web Analytics\n",
    "o\tSentiment analysis over time\n",
    "o\tTracking trends in search engine queries\n",
    "o\tAnalyzing user behavior on websites\n",
    "8.\t📦 Supply Chain and Logistics\n",
    "o\tDemand prediction for inventory management\n",
    "o\tRoute optimization for delivery services\n",
    "o\tReducing stockout risks\n",
    "________________________________________\n",
    "🚀 Advanced Techniques Used in Time Series Analysis:\n",
    "•\tARIMA (Autoregressive Integrated Moving Average)\n",
    "•\tSARIMA (Seasonal ARIMA)\n",
    "•\tExponential Smoothing (Holt-Winters)\n",
    "•\tLSTM (Long Short-Term Memory Neural Networks)\n",
    "•\tProphet Model (Developed by Facebook for forecasting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q2. What are some common time series patterns, and how can they be identified and interpreted?\n",
    "📊 Common Time Series Patterns and Their Interpretation\n",
    "Time series data often exhibits identifiable patterns that can be analyzed to make informed decisions and accurate forecasts. Here’s a breakdown of the most common patterns:\n",
    "________________________________________\n",
    "1. 📈 Trend Pattern\n",
    "Definition:\n",
    "A trend is the long-term upward or downward movement in a time series. It shows a persistent increase or decrease in the data over time.\n",
    "✅ Types:\n",
    "•\tUpward Trend: Continuous growth over time.\n",
    "•\tDownward Trend: Gradual decline over time.\n",
    "•\tStationary/Flat Trend: No noticeable increase or decrease.\n",
    "🔍 Identification:\n",
    "•\tVisual inspection: Plot the data and observe the overall direction.\n",
    "•\tStatistical methods: Linear regression or moving averages can highlight trends.\n",
    "📚 Interpretation:\n",
    "•\tUpward trend → Growing demand, positive business growth.\n",
    "•\tDownward trend → Declining sales, negative business outlook.\n",
    "________________________________________\n",
    "2. 🎡 Seasonal Pattern\n",
    "Definition:\n",
    "Seasonality refers to periodic, regular, and predictable fluctuations that repeat at specific intervals (e.g., daily, monthly, yearly).\n",
    "✅ Types:\n",
    "•\tAdditive Seasonality: Seasonality magnitude remains constant over time.\n",
    "•\tMultiplicative Seasonality: Seasonality magnitude increases or decreases over time.\n",
    "🔍 Identification:\n",
    "•\tDecompose the time series (using tools like seasonal_decompose() in Python).\n",
    "•\tLook for regular peaks and troughs at fixed intervals.\n",
    "•\tAnalyze data using seasonal indices.\n",
    "📚 Interpretation:\n",
    "•\tRetail sales rise during holidays.\n",
    "•\tElectricity usage peaks in summer due to air conditioning.\n",
    "\n",
    "________________________________________\n",
    "3. 📉 Cyclic Pattern\n",
    "Definition:\n",
    "Cyclic patterns represent long-term oscillations that occur due to economic or business cycles. These patterns do not have fixed intervals like seasonal patterns.\n",
    "✅ Characteristics:\n",
    "•\tCycles are irregular and influenced by macroeconomic factors.\n",
    "•\tCan last several years (e.g., economic boom and recession).\n",
    "🔍 Identification:\n",
    "•\tPlot the data over long periods to identify cycles.\n",
    "•\tUse autocorrelation to detect cycles in the data.\n",
    "📚 Interpretation:\n",
    "•\tStock market cycles (bull and bear markets).\n",
    "•\tEconomic cycles of expansion and contraction.\n",
    "________________________________________\n",
    "4. 📊 Irregular/Residual Pattern (Noise)\n",
    "Definition:\n",
    "Irregular or random fluctuations are unpredictable variations that do not follow a trend or seasonal pattern.\n",
    "✅ Characteristics:\n",
    "•\tCaused by unexpected events (e.g., natural disasters, political changes).\n",
    "•\tOften captured in the residual component of a decomposed time series.\n",
    "🔍 Identification:\n",
    "•\tExtract residuals after trend and seasonality decomposition.\n",
    "•\tAnalyze residuals using statistical models to detect outliers.\n",
    "📚 Interpretation:\n",
    "•\tSudden drop in sales due to unforeseen events.\n",
    "•\tAnomalies in website traffic after system outages.\n",
    "________________________________________\n",
    "5. 🔁 Stationary Pattern\n",
    "Definition:\n",
    "A stationary series has a constant mean, variance, and autocorrelation over time.\n",
    "✅ Characteristics:\n",
    "•\tNo trend or seasonality.\n",
    "•\tFluctuations occur around a constant mean.\n",
    "🔍 Identification:\n",
    "•\tCheck using the Dickey-Fuller Test (ADF Test) or KPSS Test.\n",
    "•\tPlot autocorrelation function (ACF) to identify stationarity.\n",
    "📚 Interpretation:\n",
    "•\tIdeal for modeling with ARIMA models.\n",
    "•\tNon-stationary data can be differenced to achieve stationarity.\n",
    "________________________________________\n",
    "🎯 How to Identify and Interpret Time Series Patterns\n",
    "•\tVisual Inspection: Plot the time series data.\n",
    "•\tDecomposition: Break data into trend, seasonal, and residual components.\n",
    "•\tAutocorrelation Analysis: Use ACF and PACF plots to understand relationships.\n",
    "•\tStatistical Tests: Apply Dickey-Fuller or KPSS tests to check stationarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. How can time series data be preprocessed before applying analysis techniques?\n",
    "🔥 Preprocessing Time Series Data for Analysis\n",
    "Proper preprocessing is critical for accurate time series analysis and forecasting. It ensures that the data is clean, consistent, and ready for modeling. Below are key steps and techniques to preprocess time series data effectively:\n",
    "________________________________________\n",
    "📚 1. Handling Missing Values\n",
    "Problem:\n",
    "Gaps in time series data can disrupt the analysis and lead to incorrect results.\n",
    "✅ Solutions:\n",
    "•\tForward Fill: Propagate the previous value forward (ffill).\n",
    "•\tBackward Fill: Use the next available value (bfill).\n",
    "•\tLinear Interpolation: Fill missing values by interpolating between known values.\n",
    "•\tSeasonal Imputation: Replace missing values by using values from the same period in previous cycles.\n",
    "📚 Example (Pandas in Python):\n",
    "import pandas as pd\n",
    "\n",
    "# Forward fill\n",
    "data['value'] = data['value'].fillna(method='ffill')\n",
    "\n",
    "# Linear interpolation\n",
    "data['value'] = data['value'].interpolate(method='linear')\n",
    "________________________________________\n",
    "🧹 2. Handling Outliers and Anomalies\n",
    "Problem:\n",
    "Outliers can distort trend and seasonality detection.\n",
    "✅ Solutions:\n",
    "•\tZ-Score Method: Remove or transform data points with high Z-scores.\n",
    "•\tIQR (Interquartile Range): Treat values outside 1.5x IQR as outliers.\n",
    "•\tRolling Median/Mean Smoothing: Smooth outliers by replacing them with local averages.\n",
    "📚 Example:\n",
    "import numpy as np\n",
    "# Remove outliers using Z-score\n",
    "data = data[np.abs((data['value'] - data['value'].mean()) / data['value'].std()) < 3]\n",
    "________________________________________\n",
    "📊 3. Resampling and Aggregation\n",
    "Problem:\n",
    "Time series data often needs to be aggregated or resampled to a different frequency.\n",
    "✅ Solutions:\n",
    "•\tUpsampling: Increase frequency (e.g., daily → hourly).\n",
    "•\tDownsampling: Decrease frequency (e.g., daily → weekly).\n",
    "•\tAggregation Methods: Sum, mean, median, etc.\n",
    "📚 Example:\n",
    "python\n",
    "\n",
    "# Resample to weekly data using mean\n",
    "data_resampled = data.resample('W').mean()\n",
    "________________________________________\n",
    "📏 4. Detrending and Deseasonalizing\n",
    "Problem:\n",
    "Trend and seasonality can interfere with some time series models.\n",
    "✅ Solutions:\n",
    "•\tDifferencing: Remove trends by subtracting consecutive observations.\n",
    "•\tSeasonal Decomposition: Use additive or multiplicative decomposition to isolate trend, seasonality, and residuals.\n",
    "📚 Example:\n",
    "python\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Decompose time series\n",
    "result = seasonal_decompose(data['value'], model='additive', period=12)\n",
    "data['deseasonalized'] = data['value'] - result.seasonal\n",
    "________________________________________\n",
    "📈 5. Checking and Enforcing Stationarity\n",
    "Problem:\n",
    "Most models (e.g., ARIMA) assume that the time series is stationary.\n",
    "✅ Solutions:\n",
    "•\tDifferencing: Apply first or second-order differencing to stabilize the mean.\n",
    "•\tLog or Box-Cox Transformation: Stabilize variance over time.\n",
    "📚 Example:\n",
    "python\n",
    "# First-order differencing\n",
    "data['diff'] = data['value'].diff().dropna()\n",
    "\n",
    "# Log transformation to stabilize variance\n",
    "data['log_value'] = np.log(data['value'])\n",
    "________________________________________\n",
    "🔄 6. Handling Time Zones and Date Formats\n",
    "Problem:\n",
    "Inconsistent date formats or time zones can lead to errors.\n",
    "✅ Solutions:\n",
    "•\tConvert to Datetime Format:\n",
    "python\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "•\tSet Time Zone:\n",
    "python\n",
    "data['date'] = data['date'].dt.tz_localize('UTC')\n",
    "________________________________________\n",
    "🎯 7. Smoothing and Noise Reduction\n",
    "Problem:\n",
    "High variability can make it hard to detect underlying patterns.\n",
    "✅ Solutions:\n",
    "•\tMoving Average Smoothing:\n",
    "python\n",
    "data['smoothed'] = data['value'].rolling(window=3).mean()\n",
    "•\tExponential Smoothing:\n",
    "python\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "model = ExponentialSmoothing(data['value']).fit()\n",
    "data['smoothed'] = model.fittedvalues\n",
    "________________________________________\n",
    "📌 8. Feature Engineering for Time Series\n",
    "Problem:\n",
    "Raw time series data may lack explanatory power.\n",
    "✅ Solutions:\n",
    "•\tCreate lag features (value(t-1))\n",
    "•\tAdd rolling statistics (mean, variance, etc.)\n",
    "•\tEncode seasonal indicators (day, month, holiday)\n",
    "📚 Example:\n",
    "data['lag_1'] = data['value'].shift(1)\n",
    "data['rolling_mean'] = data['value'].rolling(window=7).mean()\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "________________________________________\n",
    "🚀 Summary of Preprocessing Steps:\n",
    "1.\tHandle missing values.\n",
    "2.\tIdentify and remove outliers.\n",
    "3.\tResample and aggregate data.\n",
    "4.\tDetrend and deseasonalize if necessary.\n",
    "5.\tCheck and enforce stationarity.\n",
    "6.\tNormalize or scale data if required.\n",
    "7.\tSmooth noisy data to highlight trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q4. How can time series forecasting be used in business decision-making, and what are some common challenges and limitations?\n",
    "\n",
    "📊 Time Series Forecasting in Business Decision-Making\n",
    "Time series forecasting helps businesses predict future outcomes based on historical data. By identifying trends, seasonal patterns, and anomalies, companies can make informed decisions, optimize processes, and reduce uncertainty.\n",
    "________________________________________\n",
    "🎯 1. Demand and Inventory Management\n",
    "✅ Use Case:\n",
    "•\tForecast future demand to avoid stockouts or overstocking.\n",
    "•\tPlan inventory and supply chain logistics based on seasonal fluctuations.\n",
    "📚 Business Impact:\n",
    "•\tReduce holding costs and minimize lost sales.\n",
    "•\tOptimize warehouse space and supplier contracts.\n",
    "________________________________________\n",
    "💸 2. Sales and Revenue Forecasting\n",
    "✅ Use Case:\n",
    "•\tPredict future revenue based on historical sales data.\n",
    "•\tModel the impact of marketing campaigns on sales.\n",
    "📚 Business Impact:\n",
    "•\tSet realistic sales targets and allocate resources efficiently.\n",
    "•\tPlan promotions and discount strategies for peak seasons.\n",
    "________________________________________\n",
    "📆 3. Financial Planning and Budgeting\n",
    "✅ Use Case:\n",
    "•\tForecast cash flow, expenses, and profit margins.\n",
    "•\tAnticipate capital requirements and investment needs.\n",
    "📚 Business Impact:\n",
    "•\tEnsure liquidity and avoid financial bottlenecks.\n",
    "•\tPlan capital investments and budget allocation effectively.\n",
    "________________________________________\n",
    "⚙️ 4. Predictive Maintenance and Operations\n",
    "✅ Use Case:\n",
    "•\tAnalyze sensor data to predict equipment failure.\n",
    "•\tPlan preventive maintenance schedules.\n",
    "📚 Business Impact:\n",
    "•\tReduce downtime and maintenance costs.\n",
    "•\tImprove operational efficiency and resource utilization.\n",
    "________________________________________\n",
    "📡 5. Customer Retention and Churn Prediction\n",
    "✅ Use Case:\n",
    "•\tIdentify customers at risk of churn using behavioral data.\n",
    "•\tForecast subscription renewals and attrition rates.\n",
    "📚 Business Impact:\n",
    "•\tDesign targeted retention campaigns.\n",
    "•\tIncrease customer lifetime value (CLV) and reduce churn.\n",
    "________________________________________\n",
    "📊 6. Market and Competitor Analysis\n",
    "✅ Use Case:\n",
    "•\tAnalyze historical market trends to predict future movements.\n",
    "•\tForecast competitor behavior and market share.\n",
    "📚 Business Impact:\n",
    "•\tGain a competitive edge by anticipating market changes.\n",
    "•\tDevelop proactive marketing and product strategies.\n",
    "________________________________________\n",
    "🌦️ 7. Resource and Workforce Planning\n",
    "✅ Use Case:\n",
    "•\tForecast staffing requirements based on workload patterns.\n",
    "•\tPlan for seasonal fluctuations in labor demand.\n",
    "📚 Business Impact:\n",
    "•\tOptimize workforce allocation and reduce overtime costs.\n",
    "•\tImprove employee satisfaction by maintaining workload balance.\n",
    "________________________________________\n",
    "🛒 8. Pricing and Discount Strategies\n",
    "✅ Use Case:\n",
    "•\tPredict the impact of price changes on sales volume.\n",
    "•\tModel the effects of discount campaigns on revenue.\n",
    "📚 Business Impact:\n",
    "•\tMaximize revenue by setting optimal prices.\n",
    "•\tImprove margins while maintaining competitiveness.\n",
    "________________________________________\n",
    "⚠️ Challenges and Limitations of Time Series Forecasting\n",
    "________________________________________\n",
    "1. 📉 Data Quality and Incompleteness\n",
    "❗️ Challenge:\n",
    "Missing, inconsistent, or noisy data can reduce forecast accuracy.\n",
    "🔧 Solution:\n",
    "•\tClean and preprocess data (handle missing values and outliers).\n",
    "•\tUse interpolation or imputation techniques.\n",
    "________________________________________\n",
    "2. 🔄 Non-Stationarity and Structural Changes\n",
    "❗️ Challenge:\n",
    "Changes in business processes, consumer behavior, or external factors can make historical patterns obsolete.\n",
    "🔧 Solution:\n",
    "•\tUse differencing or transformations to make the data stationary.\n",
    "•\tApply rolling windows or adaptive models to account for changes.\n",
    "________________________________________\n",
    "3. 🕰️ Seasonality and Irregular Cycles\n",
    "❗️ Challenge:\n",
    "Unexpected seasonal variations or cyclical patterns can distort forecasts.\n",
    "🔧 Solution:\n",
    "•\tUse models like SARIMA or Prophet that account for seasonality.\n",
    "•\tRegularly update models with the latest data.\n",
    "________________________________________\n",
    "4. 🧩 Overfitting and Model Complexity\n",
    "❗️ Challenge:\n",
    "Complex models may overfit the data, leading to poor generalization.\n",
    "🔧 Solution:\n",
    "•\tUse cross-validation to tune model parameters.\n",
    "•\tImplement simpler models and gradually increase complexity.\n",
    "________________________________________\n",
    "5. ⚡️ External and Unpredictable Events\n",
    "❗️ Challenge:\n",
    "Events like pandemics, economic crises, or natural disasters can drastically alter trends.\n",
    "🔧 Solution:\n",
    "•\tUse scenario-based forecasting to model different possibilities.\n",
    "•\tIncorporate external variables and leading indicators.\n",
    "________________________________________\n",
    "6. 🔥 Limited Historical Data\n",
    "❗️ Challenge:\n",
    "Short time series may not capture long-term trends or seasonality.\n",
    "🔧 Solution:\n",
    "•\tUse external data sources to supplement internal data.\n",
    "•\tApply Bayesian models that work well with limited data.\n",
    "________________________________________\n",
    "7. 🧠 Difficulty in Model Interpretation\n",
    "❗️ Challenge:\n",
    "Black-box models (e.g., neural networks) lack interpretability, making it hard to explain predictions.\n",
    "🔧 Solution:\n",
    "•\tUse interpretable models like ARIMA or linear regression.\n",
    "•\tCombine machine learning with traditional models for better explainability.\n",
    "________________________________________\n",
    "🚀 Best Practices for Time Series Forecasting:\n",
    "1.\tChoose appropriate models (ARIMA, SARIMA, Prophet, LSTM) based on data characteristics.\n",
    "2.\tRegularly retrain models to adapt to changing conditions.\n",
    "3.\tMonitor model performance and update as needed.\n",
    "4.\tValidate results using cross-validation and backtesting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q5. What is ARIMA modelling, and how can it be used to forecast time series data?\n",
    "\n",
    "📈 ARIMA Modeling for Time Series Forecasting\n",
    "________________________________________\n",
    "🔍 What is ARIMA?\n",
    "ARIMA (AutoRegressive Integrated Moving Average) is a popular time series forecasting model that combines three components:\n",
    "1.\tAR (AutoRegressive):\n",
    "o\tUses past values (lags) of the series to predict future values.\n",
    "o\tOrder: ppp (number of lag terms).\n",
    "o\tExample: Yt=ϕ1Yt−1+ϕ2Yt−2+…+ϕpYt−p+ϵtY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots + \\phi_p Y_{t-p} + \\epsilon_tYt=ϕ1Yt−1+ϕ2Yt−2+…+ϕpYt−p+ϵt\n",
    "2.\tI (Integrated):\n",
    "o\tDifferencing is applied to make the series stationary (removing trends).\n",
    "o\tOrder: ddd (number of differences applied).\n",
    "o\tExample: First differencing: Yt−Yt−1Y_t - Y_{t-1}Yt−Yt−1\n",
    "3.\tMA (Moving Average):\n",
    "o\tUses past forecast errors to correct predictions.\n",
    "o\tOrder: qqq (number of lagged forecast errors).\n",
    "o\tExample: Yt=θ1ϵt−1+θ2ϵt−2+…+θqϵt−q+ϵtY_t = \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q} + \\epsilon_tYt=θ1ϵt−1+θ2ϵt−2+…+θqϵt−q+ϵt\n",
    "________________________________________\n",
    "🧠 Mathematical Representation\n",
    "The general ARIMA model is denoted as:\n",
    "ARIMA(p,d,q)ARIMA(p, d, q)ARIMA(p,d,q) \n",
    "Where:\n",
    "•\tppp – Number of lag terms (AR component).\n",
    "•\tddd – Number of differencing steps to make the data stationary.\n",
    "•\tqqq – Number of lagged forecast errors (MA component).\n",
    "Equation:\n",
    "Yt=ϕ1Yt−1+ϕ2Yt−2+…+ϕpYt−p+θ1ϵt−1+…+θqϵt−q+ϵtY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots + \\phi_p Y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\ldots + \\theta_q \\epsilon_{t-q} + \\epsilon_tYt=ϕ1Yt−1+ϕ2Yt−2+…+ϕpYt−p+θ1ϵt−1+…+θqϵt−q+ϵt \n",
    "________________________________________\n",
    "🛠️ Steps to Apply ARIMA for Time Series Forecasting\n",
    "________________________________________\n",
    "📚 Step 1: Make the Series Stationary\n",
    "✅ Why?\n",
    "ARIMA assumes that the data is stationary (constant mean and variance over time).\n",
    "👉 Techniques:\n",
    "•\tDifferencing: Apply first or second-order differencing.\n",
    "•\tLog Transformation: Stabilize variance.\n",
    "•\tADF Test (Augmented Dickey-Fuller): Check stationarity.\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform ADF test\n",
    "result = adfuller(data['value'])\n",
    "print(f'p-value: {result[1]}')  # p-value < 0.05 means stationary\n",
    "________________________________________\n",
    "📊 Step 2: Identify ARIMA Orders (p, d, q)\n",
    "✅ How?\n",
    "•\tp (AR Order): Look at PACF (Partial Autocorrelation Function).\n",
    "•\tq (MA Order): Look at ACF (Autocorrelation Function).\n",
    "•\td (Differencing Order): Check how many times differencing is required to achieve stationarity.\n",
    "python\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot ACF and PACF to determine p and q\n",
    "plot_acf(data['value'])\n",
    "plot_pacf(data['value'])\n",
    "plt.show()\n",
    "________________________________________\n",
    "⚙️ Step 3: Fit the ARIMA Model\n",
    "✅ Model Selection:\n",
    "•\tUse ARIMA(p, d, q) where p, d, and q are chosen based on ACF/PACF plots and differencing.\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Define and fit the ARIMA model\n",
    "model = ARIMA(data['value'], order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(model_fit.summary())\n",
    "________________________________________\n",
    "📈 Step 4: Make Predictions\n",
    "✅ Forecast Future Values:\n",
    "•\tPredict future time periods using the fitted model.\n",
    "python\n",
    "\n",
    "# Forecast 12 periods into the future\n",
    "forecast = model_fit.forecast(steps=12)\n",
    "print(forecast)\n",
    "________________________________________\n",
    "📡 Step 5: Model Evaluation\n",
    "✅ Measure Forecast Accuracy:\n",
    "•\tRMSE (Root Mean Square Error):\n",
    "•\tMAE (Mean Absolute Error):\n",
    "python\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "actual = data['value'][-12:]  # Actual values for comparison\n",
    "predicted = forecast\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "mae = mean_absolute_error(actual, predicted)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}, MAE: {mae:.2f}')\n",
    "________________________________________\n",
    "🚀 Advanced ARIMA Variants\n",
    "________________________________________\n",
    "1. 📡 SARIMA (Seasonal ARIMA)\n",
    "✅ Handles Seasonal Patterns:\n",
    "Adds seasonal terms to ARIMA for periodic patterns.\n",
    "SARIMA(p,d,q)×(P,D,Q,s)SARIMA(p, d, q) \\times (P, D, Q, s)SARIMA(p,d,q)×(P,D,Q,s) \n",
    "Where:\n",
    "•\tP,D,QP, D, QP,D,Q are seasonal orders.\n",
    "•\tsss is the seasonal period (e.g., 12 for monthly data).\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Define and fit SARIMA model\n",
    "model = SARIMAX(data['value'], order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "model_fit = model.fit()\n",
    "________________________________________\n",
    "2. 🔥 Auto-ARIMA (Automated Model Selection)\n",
    "✅ Automates p, d, q Selection:\n",
    "python\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Auto-ARIMA to identify optimal (p, d, q)\n",
    "auto_model = auto_arima(data['value'], seasonal=False, trace=True)\n",
    "print(auto_model.summary())\n",
    "________________________________________\n",
    "⚠️ Challenges and Limitations of ARIMA\n",
    "________________________________________\n",
    "1. 📉 Stationarity Assumption\n",
    "•\tARIMA works best when data is stationary.\n",
    "•\tMultiple differencing can sometimes lead to overfitting.\n",
    "________________________________________\n",
    "2. 🕰️ Difficulty Handling Seasonality\n",
    "•\tStandard ARIMA does not handle seasonality well. SARIMA or other models are required for seasonal data.\n",
    "________________________________________\n",
    "3. 🚀 Model Complexity\n",
    "•\tPoor choice of parameters can lead to overfitting or underfitting.\n",
    "•\tGrid search or Auto-ARIMA can help identify optimal parameters.\n",
    "________________________________________\n",
    "4. 📡 Limited Ability to Handle External Variables\n",
    "•\tARIMA cannot incorporate external factors easily. Use SARIMAX or VAR models for multivariate forecasting.\n",
    "________________________________________\n",
    "🎯 When to Use ARIMA?\n",
    "•\tWhen data is univariate and exhibits trend or patterns over time.\n",
    "•\tWhen stationarity can be achieved through differencing.\n",
    "•\tFor short-to-medium-term forecasts where explainability is needed.\n",
    "________________________________________\n",
    "💡 Summary\n",
    "•\tARIMA is a powerful tool for univariate time series forecasting.\n",
    "•\tIdentify appropriate orders of ARIMA using ACF/PACF and differencing.\n",
    "•\tEvaluate model performance using RMSE, MAE, and other metrics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots help in identifying the order of ARIMA models?\n",
    "\n",
    "📊 Understanding ACF and PACF in ARIMA Model Selection\n",
    "________________________________________\n",
    "🔍 What are ACF and PACF?\n",
    "________________________________________\n",
    "📡 1. Autocorrelation Function (ACF)\n",
    "✅ Definition:\n",
    "ACF measures the correlation between the time series and its lagged values over different time intervals. It helps identify how past values influence the current value of the series.\n",
    "________________________________________\n",
    "📏 Formula:\n",
    "ACF(k)=Cov(Yt,Yt−k)Var(Yt)ACF(k) = \\frac{\\text{Cov}(Y_t, Y_{t-k})}{\\text{Var}(Y_t)}ACF(k)=Var(Yt)Cov(Yt,Yt−k) \n",
    "Where:\n",
    "•\tYtY_tYt is the time series.\n",
    "•\tkkk is the lag.\n",
    "________________________________________\n",
    "✅ Interpretation:\n",
    "•\tACF shows the degree of correlation between YtY_tYt and Yt−kY_{t-k}Yt−k for different lags.\n",
    "•\tA slow decline in ACF suggests the presence of an AR (AutoRegressive) component.\n",
    "•\tA sudden drop after lag qqq suggests a MA (Moving Average) component.\n",
    "________________________________________\n",
    "✅ Plot Interpretation:\n",
    "•\tX-axis: Lag (k)\n",
    "•\tY-axis: Correlation coefficient\n",
    "•\tSignificance bands: Confidence intervals (usually 95%) to assess whether a correlation is statistically significant.\n",
    "________________________________________\n",
    "📡 2. Partial Autocorrelation Function (PACF)\n",
    "✅ Definition:\n",
    "PACF measures the correlation between the series and its lag after removing the influence of intermediate lags. It isolates the direct relationship between the series and its lagged values.\n",
    "________________________________________\n",
    "📏 Formula:\n",
    "PACF(k)=Correlation between Yt and Yt−k after removing effects of lags 1, 2, ..., k-1.PACF(k) = \\text{Correlation between } Y_t \\text{ and } Y_{t-k} \\text{ after removing effects of lags 1, 2, ..., k-1.}PACF(k)=Correlation between Yt and Yt−k after removing effects of lags 1, 2, ..., k-1. \n",
    "✅ Interpretation:\n",
    "•\tPACF focuses on the direct effect of lagged observations.\n",
    "•\tA significant spike at lag ppp in PACF indicates an AR(p) component.\n",
    "•\tPACF cuts off after lag ppp, suggesting an autoregressive model.\n",
    "________________________________________\n",
    "✅ Plot Interpretation:\n",
    "•\tX-axis: Lag (k)\n",
    "•\tY-axis: Partial correlation\n",
    "•\tSignificance bands: Confidence intervals to assess significance.\n",
    "________________________________________\n",
    "🛠️ How ACF and PACF Help in ARIMA Order Selection\n",
    "________________________________________\n",
    "📈 Identifying AR (AutoRegressive) Order — p\n",
    "✅ Check PACF:\n",
    "•\tIf PACF shows a sharp cut-off after lag ppp and ACF tails off gradually, an AR(p) model is suggested.\n",
    "•\tPACF spikes up to lag ppp indicate the order of the AR component.\n",
    "👉 Guideline:\n",
    "•\tPACF cuts off after lag ppp → AR(p)\n",
    "•\tACF decays exponentially or sinusoidally → AR process\n",
    "________________________________________\n",
    "📊 Identifying MA (Moving Average) Order — q\n",
    "✅ Check ACF:\n",
    "•\tIf ACF shows a sharp cut-off after lag qqq and PACF tails off, an MA(q) model is suggested.\n",
    "•\tACF spikes up to lag qqq indicate the order of the MA component.\n",
    "👉 Guideline:\n",
    "•\tACF cuts off after lag qqq → MA(q)\n",
    "•\tPACF decays gradually → MA process\n",
    "________________________________________\n",
    "🔄 Identifying Differencing Order — d\n",
    "✅ Stationarity Check:\n",
    "•\tIf ACF and PACF show slow decay, the series may need differencing to remove trend or seasonality.\n",
    "•\tUse the Augmented Dickey-Fuller (ADF) test to check stationarity.\n",
    "👉 Guideline:\n",
    "•\tApply differencing until ACF and PACF plots show no trend.\n",
    "________________________________________\n",
    "📚 Summary of Model Identification Using ACF and PACF\n",
    "________________________________________\n",
    "🤖 1. AR(p) Model\n",
    "•\tACF: Tails off gradually.\n",
    "•\tPACF: Sharp cut-off after lag ppp.\n",
    "________________________________________\n",
    "⚡️ 2. MA(q) Model\n",
    "•\tACF: Sharp cut-off after lag qqq.\n",
    "•\tPACF: Tails off gradually.\n",
    "________________________________________\n",
    "🔄 3. ARMA(p, q) Model\n",
    "•\tACF: Tails off.\n",
    "•\tPACF: Tails off.\n",
    "________________________________________\n",
    "🕰️ 4. ARIMA(p, d, q) Model\n",
    "•\tDifferencing: Apply differencing if ACF shows a trend.\n",
    "•\tUse ACF/PACF plots on differenced data to identify ppp and qqq.\n",
    "________________________________________\n",
    "🔥 Visual Example of ACF and PACF in Python\n",
    "python\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Load time series data\n",
    "data = pd.read_csv('time_series_data.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "# Plot ACF and PACF\n",
    "plot_acf(data['value'], lags=20)\n",
    "plt.title('Autocorrelation (ACF) Plot')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(data['value'], lags=20)\n",
    "plt.title('Partial Autocorrelation (PACF) Plot')\n",
    "plt.show()\n",
    "________________________________________\n",
    "🚀 How to Choose ARIMA Orders (p, d, q) Based on ACF and PACF\n",
    "Scenario\tACF Pattern\tPACF Pattern\tSuggested Model\n",
    "AR Model\tGradual decay\tSharp cut-off at lag ppp\tAR(p)\n",
    "MA Model\tSharp cut-off at lag qqq\tGradual decay\tMA(q)\n",
    "ARMA Model\tGradual decay\tGradual decay\tARMA(p, q)\n",
    "Differencing Required\tACF decays slowly\tPACF shows trend\tApply differencing\n",
    "________________________________________\n",
    "⚠️ Common Challenges in ACF and PACF Interpretation\n",
    "________________________________________\n",
    "1. 📉 Over-Differencing\n",
    "•\tApplying too much differencing can remove valuable information and lead to overfitting.\n",
    "2. 🔄 Identifying Mixed Models (ARMA)\n",
    "•\tWhen both ACF and PACF tail off, it may indicate an ARMA process that requires a combination of AR and MA terms.\n",
    "3. 📊 Handling Seasonality\n",
    "•\tFor seasonal data, look at seasonal lags and apply SARIMA to capture periodicity.\n",
    "________________________________________\n",
    "🎯 Pro Tip:\n",
    "•\tStart with Auto-ARIMA to automate the process and fine-tune based on ACF and PACF insights.\n",
    "python\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Auto-ARIMA to identify optimal (p, d, q)\n",
    "auto_model = auto_arima(data['value'], seasonal=False, trace=True)\n",
    "print(auto_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q7. What are the assumptions of ARIMA models, and how can they be tested for in practice?\n",
    "\n",
    "📊 Assumptions of ARIMA Models and How to Test Them\n",
    "________________________________________\n",
    "📚 What is ARIMA?\n",
    "ARIMA (AutoRegressive Integrated Moving Average) models are used for time series forecasting by combining:\n",
    "•\tAR (AutoRegressive): Uses past values to predict future values.\n",
    "•\tI (Integrated): Differencing to make the series stationary.\n",
    "•\tMA (Moving Average): Uses past error terms to correct predictions.\n",
    "________________________________________\n",
    "✅ Key Assumptions of ARIMA Models\n",
    "________________________________________\n",
    "1. 📈 Stationarity of the Time Series\n",
    "✅ Assumption:\n",
    "The time series should be stationary, meaning its statistical properties (mean, variance, and autocorrelation) remain constant over time.\n",
    "________________________________________\n",
    "📏 Why It’s Important:\n",
    "•\tARIMA models rely on lagged values and previous error terms. If the series is non-stationary, predictions may be inaccurate.\n",
    "________________________________________\n",
    "🛠️ How to Test for Stationarity:\n",
    "•\tAugmented Dickey-Fuller (ADF) Test:\n",
    "Null Hypothesis: Series is non-stationary.\n",
    "Alternative Hypothesis: Series is stationary.\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(data['value'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"The series is stationary.\")\n",
    "else:\n",
    "    print(\"The series is non-stationary. Differencing is required.\")\n",
    "•\tKPSS (Kwiatkowski-Phillips-Schmidt-Shin) Test:\n",
    "Null Hypothesis: Series is stationary.\n",
    "Alternative Hypothesis: Series is non-stationary.\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "kpss_test = kpss(data['value'], regression='c')\n",
    "print(f'KPSS Statistic: {kpss_test[0]}')\n",
    "print(f'p-value: {kpss_test[1]}')\n",
    "\n",
    "if kpss_test[1] > 0.05:\n",
    "    print(\"The series is stationary.\")\n",
    "else:\n",
    "    print(\"The series is non-stationary. Differencing is required.\")\n",
    "________________________________________\n",
    "🔄 How to Handle Non-Stationarity:\n",
    "•\tApply differencing using:\n",
    "python\n",
    "\n",
    "data['differenced'] = data['value'].diff().dropna()\n",
    "•\tCheck ADF test again after differencing.\n",
    "________________________________________\n",
    "________________________________________\n",
    "2. 🔁 No Autocorrelation in Residuals\n",
    "✅ Assumption:\n",
    "Residuals (errors) should be uncorrelated (white noise), meaning they follow a random pattern.\n",
    "________________________________________\n",
    "📏 Why It’s Important:\n",
    "•\tIf residuals exhibit correlation, the model hasn’t captured all the patterns, leading to biased forecasts.\n",
    "________________________________________\n",
    "🛠️ How to Test for Autocorrelation:\n",
    "•\tLjung-Box Test:\n",
    "Null Hypothesis: Residuals are independently distributed.\n",
    "Alternative Hypothesis: Residuals show autocorrelation.\n",
    "python\n",
    "\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Check residuals of ARIMA model\n",
    "residuals = fitted_model.resid\n",
    "ljung_box_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "print(ljung_box_test)\n",
    "\n",
    "if ljung_box_test['lb_pvalue'].values[0] > 0.05:\n",
    "    print(\"Residuals show no autocorrelation. Assumption satisfied.\")\n",
    "else:\n",
    "    print(\"Residuals show autocorrelation. Model may need improvement.\")\n",
    "•\tACF Plot of Residuals:\n",
    "python\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(residuals, lags=20)\n",
    "plt.title(\"ACF Plot of Residuals\")\n",
    "plt.show()\n",
    "________________________________________\n",
    "________________________________________\n",
    "3. 📊 Constant Mean and Variance (Homoscedasticity)\n",
    "✅ Assumption:\n",
    "The residuals should have constant variance over time (no heteroscedasticity).\n",
    "________________________________________\n",
    "📏 Why It’s Important:\n",
    "•\tIf the variance changes over time, it can affect the reliability of the model’s predictions.\n",
    "________________________________________\n",
    "🛠️ How to Test for Homoscedasticity:\n",
    "•\tPlot Residuals:\n",
    "python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.title(\"Residuals over Time\")\n",
    "plt.show()\n",
    "•\tARCH Test (Autoregressive Conditional Heteroscedasticity):\n",
    "Null Hypothesis: Residuals have constant variance.\n",
    "Alternative Hypothesis: Residuals show changing variance.\n",
    "python\n",
    "\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "arch_test = het_arch(residuals)\n",
    "print(f'ARCH Test Statistic: {arch_test[0]}')\n",
    "print(f'p-value: {arch_test[1]}')\n",
    "\n",
    "if arch_test[1] > 0.05:\n",
    "    print(\"Residuals have constant variance. Assumption satisfied.\")\n",
    "else:\n",
    "    print(\"Heteroscedasticity detected. Model may need improvement.\")\n",
    "________________________________________\n",
    "________________________________________\n",
    "4. 🔄 Normality of Residuals\n",
    "✅ Assumption:\n",
    "Residuals should be normally distributed with a mean of zero.\n",
    "________________________________________\n",
    "📏 Why It’s Important:\n",
    "•\tIf residuals are not normal, confidence intervals and prediction intervals may be unreliable.\n",
    "________________________________________\n",
    "🛠️ How to Test for Normality:\n",
    "•\tShapiro-Wilk Test:\n",
    "Null Hypothesis: Residuals follow a normal distribution.\n",
    "Alternative Hypothesis: Residuals are not normal.\n",
    "python\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "shapiro_test = shapiro(residuals)\n",
    "print(f'Shapiro-Wilk Test Statistic: {shapiro_test[0]}')\n",
    "print(f'p-value: {shapiro_test[1]}')\n",
    "\n",
    "if shapiro_test[1] > 0.05:\n",
    "    print(\"Residuals are normally distributed. Assumption satisfied.\")\n",
    "else:\n",
    "    print(\"Residuals are not normally distributed. Model may need adjustment.\")\n",
    "•\tQQ Plot:\n",
    "python\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title(\"QQ Plot of Residuals\")\n",
    "plt.show()\n",
    "________________________________________\n",
    "________________________________________\n",
    "5. ⏳ Sufficient Number of Observations\n",
    "✅ Assumption:\n",
    "ARIMA models perform best with a sufficiently large amount of historical data (at least 50 observations).\n",
    "________________________________________\n",
    "📏 Why It’s Important:\n",
    "•\tToo few observations can lead to unreliable parameter estimates and poor forecasting.\n",
    "________________________________________\n",
    "🛠️ How to Check:\n",
    "•\tEnsure at least 50–100 observations are available for reliable model performance.\n",
    "python\n",
    "\n",
    "print(f\"Number of observations: {len(data)}\")\n",
    "________________________________________\n",
    "🎯 Summary: Key Assumptions and Tests\n",
    "________________________________________\n",
    "Assumption\tTest/Check\tNull Hypothesis\tThreshold\n",
    "Stationarity\tADF / KPSS Test\tSeries is non-stationary / stationary\tp < 0.05\n",
    "No Autocorrelation in Residuals\tLjung-Box Test\tResiduals are uncorrelated\tp > 0.05\n",
    "Constant Variance\tARCH Test\tResiduals have constant variance\tp > 0.05\n",
    "Normality of Residuals\tShapiro-Wilk / QQ Plot\tResiduals are normally distributed\tp > 0.05\n",
    "Sufficient Observations\tCheck Data Size\tAt least 50-100 observations\tN ≥ 50\n",
    "________________________________________\n",
    "🚀 Pro Tip:\n",
    "•\tUse Auto-ARIMA for initial order selection, but manually validate assumptions for better results.\n",
    "•\tVisualize residuals, ACF/PACF, and conduct diagnostic tests after fitting the model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Suppose you have monthly sales data for a retail store for the past three years. Which type of time series model would you recommend for forecasting future sales, and why?\n",
    "\n",
    "🛒 Q8: Recommended Time Series Model for Retail Store Sales Forecasting\n",
    "________________________________________\n",
    "📊 Scenario:\n",
    "•\tData: Monthly sales data for a retail store over the past 3 years.\n",
    "•\tGoal: Forecast future sales.\n",
    "________________________________________\n",
    "🎯 Recommended Model:\n",
    "1. SARIMA (Seasonal ARIMA) – Best Choice\n",
    "✅ Why:\n",
    "•\tSeasonality: Since the data is monthly, it is likely to exhibit seasonal patterns (e.g., higher sales during festivals, holidays, or end of the year).\n",
    "•\tTrend & Cyclic Patterns: SARIMA models can handle trends, cycles, and seasonal variations.\n",
    "•\tDifferencing: Can address stationarity issues through differencing.\n",
    "________________________________________\n",
    "📚 Model Description:\n",
    "SARIMA is an extension of ARIMA that accounts for seasonal patterns.\n",
    "•\tARIMA(p, d, q): Handles trend and non-seasonal components.\n",
    "o\tp = Order of AutoRegression (AR)\n",
    "o\td = Degree of differencing (to achieve stationarity)\n",
    "o\tq = Order of Moving Average (MA)\n",
    "•\tSeasonal Component (P, D, Q, m):\n",
    "o\tP = Seasonal order of AR\n",
    "o\tD = Seasonal differencing\n",
    "o\tQ = Seasonal order of MA\n",
    "o\tm = Seasonal period (e.g., 12 for monthly data)\n",
    "________________________________________\n",
    "🔎 Example:\n",
    "For monthly data with annual seasonality:\n",
    "•\tSARIMA(p, d, q)(P, D, Q, 12)\n",
    "________________________________________\n",
    "🚀 Steps to Build SARIMA Model:\n",
    "1.\tCheck for Stationarity: \n",
    "o\tUse ADF or KPSS tests.\n",
    "o\tApply differencing if required.\n",
    "2.\tIdentify p, d, q using ACF and PACF.\n",
    "3.\tIdentify Seasonal Parameters (P, D, Q) using Seasonal ACF and PACF.\n",
    "4.\tFit SARIMA Model:\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Define SARIMA model\n",
    "model = SARIMAX(data['sales'], \n",
    "                order=(p, d, q), \n",
    "                seasonal_order=(P, D, Q, 12),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False)\n",
    "\n",
    "# Fit model\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast for next 12 months\n",
    "forecast = fitted_model.forecast(steps=12)\n",
    "print(forecast)\n",
    "________________________________________\n",
    "📈 2. Prophet (Alternative for Irregular Seasonality)\n",
    "✅ Why:\n",
    "•\tSuitable for time series data with holiday effects, irregular seasonality, and trend changes.\n",
    "•\tHandles holidays and special promotions that may affect sales.\n",
    "________________________________________\n",
    "🔎 Example:\n",
    "python\n",
    "\n",
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "data = data.rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "\n",
    "# Define model\n",
    "model = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
    "model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "\n",
    "# Fit and forecast\n",
    "model.fit(data)\n",
    "future = model.make_future_dataframe(periods=12, freq='M')\n",
    "forecast = model.predict(future)\n",
    "model.plot(forecast)\n",
    "________________________________________\n",
    "⚡ 3. Exponential Smoothing (ETS) – For Simpler Patterns\n",
    "✅ Why:\n",
    "•\tSuitable for data with trend and seasonality but fewer complex interactions.\n",
    "•\tSimpler to implement and interpret.\n",
    "________________________________________\n",
    "🔎 Example:\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Define model\n",
    "model = ExponentialSmoothing(data['sales'], \n",
    "                              trend='add', \n",
    "                              seasonal='add', \n",
    "                              seasonal_periods=12)\n",
    "\n",
    "# Fit and forecast\n",
    "fitted_model = model.fit()\n",
    "forecast = fitted_model.forecast(steps=12)\n",
    "________________________________________\n",
    "🧐 Model Comparison:\n",
    "Model\tWhen to Use\tPros\tCons\n",
    "SARIMA\tSeasonal + Trend Data\tAccurate, Handles Seasonality\tComplex Parameter Tuning\n",
    "Prophet\tIrregular Seasonality + Holidays\tHandles Trend Shifts, Holidays\tLess Effective for Pure ARIMA\n",
    "Exponential Smoothing\tSimpler Seasonal + Trend\tEasy to Implement\tLess Accurate for Complex Data\n",
    "________________________________________\n",
    "🎯 Final Recommendation:\n",
    "•\tSARIMA: Preferred for structured seasonal data like monthly sales.\n",
    "•\tProphet: Ideal if sales show irregular patterns or event-driven fluctuations.\n",
    "\n",
    "Q9. What are some of the limitations of time series analysis? Provide an example of a scenario where the limitations of time series analysis may be particularly relevant.\n",
    "\n",
    "⏰ Q9: Limitations of Time Series Analysis with Example\n",
    "________________________________________\n",
    "⚡️ 1. Assumes Stationarity\n",
    "✅ Explanation:\n",
    "•\tTime series models like ARIMA assume that the underlying data is stationary (constant mean and variance over time).\n",
    "•\tIf the data shows evolving trends, structural breaks, or seasonal variations that are not adequately accounted for, the model may produce inaccurate forecasts.\n",
    "📉 Example:\n",
    "•\tStock Prices: Stock market prices often exhibit non-stationarity due to sudden market events (e.g., earnings reports, global crises).\n",
    "•\tImpact: ARIMA might fail to capture sudden shifts or volatility, making it unsuitable for modeling financial data without appropriate differencing or transformations.\n",
    "________________________________________\n",
    "📚 2. Limited in Handling External Variables\n",
    "✅ Explanation:\n",
    "•\tStandard time series models focus only on past values of the target variable, ignoring the impact of external factors (exogenous variables).\n",
    "•\tThese factors may significantly affect outcomes but remain unaccounted for in pure time series models.\n",
    "📉 Example:\n",
    "•\tRetail Sales: Holiday promotions, marketing campaigns, and inflation may influence retail sales.\n",
    "•\tImpact: A model that doesn't account for these variables may under- or overestimate future sales.\n",
    "________________________________________\n",
    "⏳ 3. Poor at Capturing Long-Term Dependencies\n",
    "✅ Explanation:\n",
    "•\tTime series models often struggle to capture long-term trends or dependencies beyond a certain lag.\n",
    "•\tAutoregressive models focus on short-term relationships, making them unsuitable for data with deep historical influences.\n",
    "📉 Example:\n",
    "•\tClimate Data: Long-term climate patterns (like El Niño) may have effects spanning years or decades.\n",
    "•\tImpact: ARIMA or SARIMA models may miss long-term shifts, underestimating the impact of climate trends.\n",
    "________________________________________\n",
    "📉 4. Sensitivity to Outliers and Noise\n",
    "✅ Explanation:\n",
    "•\tTime series models are highly sensitive to outliers, missing data, and noise, which can distort model accuracy.\n",
    "•\tOutliers can significantly influence the model's behavior, leading to biased forecasts.\n",
    "📉 Example:\n",
    "•\tCOVID-19 Impact on Demand: Abrupt drops or spikes in consumer demand during COVID-19 caused unexpected trends.\n",
    "•\tImpact: Models trained on pre-pandemic data struggled to adapt to the new post-pandemic demand patterns.\n",
    "________________________________________\n",
    "📈 5. Ineffective with Structural Breaks\n",
    "✅ Explanation:\n",
    "•\tTime series models assume that the relationship between variables remains consistent over time.\n",
    "•\tStructural breaks (sudden shifts in the underlying process) can invalidate the model’s assumptions.\n",
    "📉 Example:\n",
    "•\tPolicy Changes in Banking: New regulations or changes in interest rates can change customer behavior abruptly.\n",
    "•\tImpact: A pre-policy change model may no longer predict outcomes effectively.\n",
    "________________________________________\n",
    "🔄 6. Assumes Linear Relationships\n",
    "✅ Explanation:\n",
    "•\tClassical time series models assume linear relationships between variables.\n",
    "•\tComplex, nonlinear relationships often require more advanced models like machine learning (e.g., LSTM, XGBoost).\n",
    "📉 Example:\n",
    "•\tCustomer Churn Prediction: Churn behavior may be influenced by multiple nonlinear factors.\n",
    "•\tImpact: ARIMA or exponential smoothing models may fail to capture complex customer behaviors.\n",
    "________________________________________\n",
    "🎯 Scenario Where Limitations are Relevant:\n",
    "🏢 Scenario: E-Commerce Sales Prediction During Festive Season\n",
    "•\tAn e-commerce company wants to forecast sales during the Diwali season.\n",
    "•\tHistorical sales data shows a consistent upward trend during the season, but: \n",
    "o\tMarketing campaigns vary in intensity.\n",
    "o\tPromotions and discounts change every year.\n",
    "o\tUnexpected competition from new players alters demand.\n",
    "⚠️ Limitations Observed:\n",
    "•\tNon-Stationarity: Sales surge unpredictably due to new promotions.\n",
    "•\tExternal Factors Ignored: ARIMA cannot account for varying marketing efforts.\n",
    "•\tOutliers/Noise: One-time flash sales create spikes that distort forecasts.\n",
    "✅ Solution:\n",
    "•\tUse models like SARIMA with exogenous variables (SARIMAX) or Prophet that account for seasonal events and external factors.\n",
    "________________________________________\n",
    "🧐 Key Takeaway:\n",
    "Time series models work best in stable, predictable environments but may struggle when external factors, structural breaks, or nonlinear patterns influence the data. Recognizing these limitations is critical for choosing the right model. 📊\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q10. Explain the difference between a stationary and non-stationary time series. How does the stationarity of a time series affect the choice of forecasting model?\n",
    "\n",
    "⏰ Q10: Stationary vs. Non-Stationary Time Series and Its Impact on Forecasting Models\n",
    "________________________________________\n",
    "📊 1. What is a Stationary Time Series?\n",
    "✅ Definition:\n",
    "•\tA time series is stationary when its statistical properties (mean, variance, and autocorrelation) remain constant over time.\n",
    "•\tNo trend, no seasonality, and fluctuations are around a constant mean.\n",
    "________________________________________\n",
    "📚 Characteristics:\n",
    "•\tConstant Mean: The average value does not change over time.\n",
    "•\tConstant Variance: The spread (variance) remains consistent.\n",
    "•\tConstant Autocorrelation: Correlation between observations depends only on the lag, not on the actual time.\n",
    "________________________________________\n",
    "📈 Examples:\n",
    "1.\tStock Price Returns: Daily percentage change in stock prices.\n",
    "2.\tResiduals from a Regressed Model: Residuals often exhibit stationary behavior after removing trend/seasonality.\n",
    "________________________________________\n",
    "🔎 How to Identify Stationarity:\n",
    "•\tVisual Inspection: Flat time series plot without upward/downward trends.\n",
    "•\tAugmented Dickey-Fuller (ADF) Test: \n",
    "o\tNull Hypothesis: Series is non-stationary.\n",
    "o\tIf p-value < 0.05, reject null and conclude stationarity.\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(data['sales'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "________________________________________\n",
    "📊 2. What is a Non-Stationary Time Series?\n",
    "✅ Definition:\n",
    "•\tA time series is non-stationary when its statistical properties change over time.\n",
    "•\tIt exhibits trends, seasonality, or both, which violate stationarity.\n",
    "________________________________________\n",
    "📚 Characteristics:\n",
    "•\tChanging Mean: Increasing or decreasing trend over time.\n",
    "•\tChanging Variance: Variability may grow or shrink.\n",
    "•\tAutocorrelation Varies: Correlation between values changes over time.\n",
    "________________________________________\n",
    "📈 Examples:\n",
    "1.\tRetail Sales: Sales data with upward trends during holiday seasons.\n",
    "2.\tTemperature Data: Rising global temperatures over decades.\n",
    "3.\tWebsite Traffic: Increasing user visits due to marketing growth.\n",
    "________________________________________\n",
    "🔎 How to Identify Non-Stationarity:\n",
    "•\tVisual Inspection: Upward/downward trend or seasonal fluctuations.\n",
    "•\tADF Test Result: High p-value (> 0.05) suggests non-stationarity.\n",
    "________________________________________\n",
    "🔄 3. How Does Stationarity Affect Model Choice?\n",
    "________________________________________\n",
    "📚 A. Models for Stationary Data:\n",
    "1.\tARIMA (p, 0, q): If data is stationary, no differencing is required (d = 0).\n",
    "o\tARIMA assumes stationary data.\n",
    "o\tDirectly models stationary series with autoregression and moving average terms.\n",
    "2.\tExponential Smoothing: For short-term stationary series with small variations.\n",
    "o\tSuitable for stable patterns.\n",
    "________________________________________\n",
    "📊 B. Models for Non-Stationary Data:\n",
    "1.\tARIMA (p, d, q):\n",
    "o\tIf data is non-stationary, differencing (d > 0) is applied to make the series stationary.\n",
    "o\tDifferencing removes trends and stabilizes variance.\n",
    "python\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(data['sales'], order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "2.\tSARIMA (p, d, q)(P, D, Q, m):\n",
    "o\tHandles seasonal non-stationarity.\n",
    "o\tSeasonal differencing (D > 0) accounts for periodic patterns.\n",
    "3.\tProphet:\n",
    "o\tAutomatically detects and models trends and seasonality.\n",
    "o\tHandles complex seasonal patterns and trend shifts.\n",
    "________________________________________\n",
    "⚡️ 4. Impact of Stationarity on Forecasting\n",
    "✅ If Data is Stationary:\n",
    "•\tEasier to model using ARMA/ARIMA.\n",
    "•\tModel parameters remain consistent over time.\n",
    "•\tForecasts are more reliable for short-term predictions.\n",
    "❗️ If Data is Non-Stationary:\n",
    "•\tDifferencing required to transform data to stationarity.\n",
    "•\tFailure to address non-stationarity leads to: \n",
    "o\tSpurious results.\n",
    "o\tOverfitting or underfitting.\n",
    "o\tPoor forecast accuracy.\n",
    "________________________________________\n",
    "📝 5. Steps to Convert Non-Stationary Data to Stationary:\n",
    "📉 A. Differencing:\n",
    "•\tSubtract the previous value from the current value.\n",
    "•\tFirst-order differencing removes linear trends.\n",
    "python\n",
    "\n",
    "data['diff_sales'] = data['sales'].diff()\n",
    "📉 B. Seasonal Differencing:\n",
    "•\tSubtract the value from the same season in the previous period.\n",
    "python\n",
    "\n",
    "data['seasonal_diff'] = data['sales'].diff(12)  # For monthly data\n",
    "📉 C. Log Transformation:\n",
    "•\tStabilizes variance for heteroscedastic data.\n",
    "python\n",
    "\n",
    "import numpy as np\n",
    "data['log_sales'] = np.log(data['sales'])\n",
    "________________________________________\n",
    "🎯 6. Key Takeaway:\n",
    "•\tStationary Data: Easier to model with ARIMA/ETS models.\n",
    "•\tNon-Stationary Data: Requires transformation (differencing, detrending) before applying models.\n",
    "•\tIdentifying and ensuring stationarity is a critical step in accurate time series forecasting. 🚀\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
